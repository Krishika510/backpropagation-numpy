{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Propagation for a Neural Network\n",
    "\n",
    "This script illustrates how backpropagation can be used to train a neural network by setting a simple binary classification problem using the MNIST dataset. You will find the function *net_backprop* in the script *fncs.py*, which requires you to implement the backpropagation algorithm. Note that you will have a few modifications since the gradients need to aggregate (by summing) the contributions from each training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow and other necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import fncs as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from sklearn.datasets import fetch_openml\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "mnist = fetch_openml('mnist_784',version=1)\n",
    "\n",
    "# Getting training and testing data:\n",
    "# We are setting up just a simple binary classification problem in which we aim to\n",
    "# properly classify the number 2.\n",
    "X, y_str = mnist[\"data\"], mnist[\"target\"]\n",
    "y = np.array([int(int(i)==2) for i in y_str])\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a neural network structure\n",
    "net = nn.net_create([784,100,1])\n",
    "\n",
    "# Training neural network:\n",
    "# Note that since I am not doing any hyper-parameter tuning, I am using the test sets for\n",
    "# validation to show how the generalization error changes as the network gets trained. \n",
    "Loss,Loss_val,mae_val = nn.net_train(net,X_train,y_train,X_test,y_test,epsilon=1e-6,NIter=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting learning curves:\n",
    "# Note that we don't observe overfitting here because the model is very simple.\n",
    "plt.plot(Loss/np.max(Loss))\n",
    "plt.plot(Loss_val/np.max(Loss_val))\n",
    "plt.legend({'Normalized Training Loss','Normalized Validation Loss'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
